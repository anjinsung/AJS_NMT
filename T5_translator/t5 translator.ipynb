{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad640e0-46fb-4ad0-bd54-375aa7df59af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "Collecting sacrebleu\n",
      "  Using cached sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1\n",
      "  Using cached huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Collecting accelerate>=0.20.3\n",
      "  Using cached accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "Collecting torch!=1.12.0,>=1.10\n",
      "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Collecting fsspec[http]<2023.9.0,>=2023.1.0\n",
      "  Using cached fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Using cached pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0\n",
      "  Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting lxml\n",
      "  Using cached lxml-4.9.3-cp310-cp310-manylinux_2_28_x86_64.whl (7.9 MB)\n",
      "Collecting portalocker\n",
      "  Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting psutil\n",
      "  Using cached psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting charset-normalizer<4.0,>=2.0\n",
      "  Using cached charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (201 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.0.5-py3-none-any.whl (123 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting triton==2.0.0\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-68.2.2-py3-none-any.whl (807 kB)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.41.2-py3-none-any.whl (64 kB)\n",
      "Collecting cmake\n",
      "  Using cached cmake-3.27.5-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
      "Collecting lit\n",
      "  Using cached lit-16.0.6-py3-none-any.whl\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Collecting six>=1.5\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, pytz, mpmath, lit, cmake, xxhash, wheel, urllib3, tzdata, typing-extensions, tqdm, tabulate, sympy, six, setuptools, regex, pyyaml, psutil, portalocker, packaging, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, numpy, networkx, multidict, MarkupSafe, lxml, idna, fsspec, frozenlist, filelock, dill, colorama, charset-normalizer, certifi, attrs, async-timeout, yarl, sacrebleu, requests, python-dateutil, pyarrow, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, multiprocess, jinja2, aiosignal, responses, pandas, nvidia-cusolver-cu11, nvidia-cudnn-cu11, huggingface-hub, aiohttp, transformers, datasets, evaluate, triton, torch, accelerate\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.99\n",
      "    Uninstalling sentencepiece-0.1.99:\n",
      "      Successfully uninstalled sentencepiece-0.1.99\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.3.3\n",
      "    Uninstalling safetensors-0.3.3:\n",
      "      Successfully uninstalled safetensors-0.3.3\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2023.3.post1\n",
      "    Uninstalling pytz-2023.3.post1:\n",
      "      Successfully uninstalled pytz-2023.3.post1\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: lit\n",
      "    Found existing installation: lit 16.0.6\n",
      "    Uninstalling lit-16.0.6:\n",
      "      Successfully uninstalled lit-16.0.6\n",
      "\u001b[33m  WARNING: The script lit is installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: cmake\n",
      "    Found existing installation: cmake 3.27.5\n",
      "    Uninstalling cmake-3.27.5:\n",
      "      Successfully uninstalled cmake-3.27.5\n",
      "\u001b[33m  WARNING: The scripts cmake, cpack and ctest are installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: xxhash\n",
      "    Found existing installation: xxhash 3.3.0\n",
      "    Uninstalling xxhash-3.3.0:\n",
      "      Successfully uninstalled xxhash-3.3.0\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.41.2\n",
      "    Uninstalling wheel-0.41.2:\n",
      "      Successfully uninstalled wheel-0.41.2\n",
      "\u001b[33m  WARNING: The script wheel is installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.5\n",
      "    Uninstalling urllib3-2.0.5:\n",
      "      Successfully uninstalled urllib3-2.0.5\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2023.3\n",
      "    Uninstalling tzdata-2023.3:\n",
      "      Successfully uninstalled tzdata-2023.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: tabulate\n",
      "    Found existing installation: tabulate 0.9.0\n",
      "    Uninstalling tabulate-0.9.0:\n",
      "      Successfully uninstalled tabulate-0.9.0\n",
      "\u001b[33m  WARNING: The script tabulate is installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "\u001b[33m  WARNING: The script isympy is installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 68.2.2\n",
      "    Uninstalling setuptools-68.2.2:\n",
      "      Successfully uninstalled setuptools-68.2.2\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2023.8.8\n",
      "    Uninstalling regex-2023.8.8:\n",
      "      Successfully uninstalled regex-2023.8.8\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.5\n",
      "    Uninstalling psutil-5.9.5:\n",
      "      Successfully uninstalled psutil-5.9.5\n",
      "  Attempting uninstall: portalocker\n",
      "    Found existing installation: portalocker 2.8.2\n",
      "    Uninstalling portalocker-2.8.2:\n",
      "      Successfully uninstalled portalocker-2.8.2\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: nvidia-nccl-cu11\n",
      "    Found existing installation: nvidia-nccl-cu11 2.14.3\n",
      "    Uninstalling nvidia-nccl-cu11-2.14.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu11-2.14.3\n",
      "  Attempting uninstall: nvidia-cufft-cu11\n",
      "    Found existing installation: nvidia-cufft-cu11 10.9.0.58\n",
      "    Uninstalling nvidia-cufft-cu11-10.9.0.58:\n",
      "      Successfully uninstalled nvidia-cufft-cu11-10.9.0.58\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu11\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu11 11.7.99\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu11-11.7.99:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu11-11.7.99\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.0\n",
      "    Uninstalling numpy-1.26.0:\n",
      "      Successfully uninstalled numpy-1.26.0\n",
      "\u001b[33m  WARNING: The script f2py is installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.1\n",
      "    Uninstalling networkx-3.1:\n",
      "      Successfully uninstalled networkx-3.1\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.0.4\n",
      "    Uninstalling multidict-6.0.4:\n",
      "      Successfully uninstalled multidict-6.0.4\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.3\n",
      "    Uninstalling MarkupSafe-2.1.3:\n",
      "      Successfully uninstalled MarkupSafe-2.1.3\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 4.9.3\n",
      "    Uninstalling lxml-4.9.3:\n",
      "      Successfully uninstalled lxml-4.9.3\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.6.0\n",
      "    Uninstalling fsspec-2023.6.0:\n",
      "      Successfully uninstalled fsspec-2023.6.0\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.4.0\n",
      "    Uninstalling frozenlist-1.4.0:\n",
      "      Successfully uninstalled frozenlist-1.4.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.12.4\n",
      "    Uninstalling filelock-3.12.4:\n",
      "      Successfully uninstalled filelock-3.12.4\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.6\n",
      "    Uninstalling colorama-0.4.6:\n",
      "      Successfully uninstalled colorama-0.4.6\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.2.0\n",
      "    Uninstalling charset-normalizer-3.2.0:\n",
      "      Successfully uninstalled charset-normalizer-3.2.0\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.7.22\n",
      "    Uninstalling certifi-2023.7.22:\n",
      "      Successfully uninstalled certifi-2023.7.22\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 4.0.3\n",
      "    Uninstalling async-timeout-4.0.3:\n",
      "      Successfully uninstalled async-timeout-4.0.3\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.9.2\n",
      "    Uninstalling yarl-1.9.2:\n",
      "      Successfully uninstalled yarl-1.9.2\n",
      "  Attempting uninstall: sacrebleu\n",
      "    Found existing installation: sacrebleu 2.3.1\n",
      "    Uninstalling sacrebleu-2.3.1:\n",
      "      Successfully uninstalled sacrebleu-2.3.1\n",
      "\u001b[33m  WARNING: The script sacrebleu is installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 13.0.0\n",
      "    Uninstalling pyarrow-13.0.0:\n",
      "      Successfully uninstalled pyarrow-13.0.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu11\n",
      "    Found existing installation: nvidia-nvtx-cu11 11.7.91\n",
      "    Uninstalling nvidia-nvtx-cu11-11.7.91:\n",
      "      Successfully uninstalled nvidia-nvtx-cu11-11.7.91\n",
      "  Attempting uninstall: nvidia-cusparse-cu11\n",
      "    Found existing installation: nvidia-cusparse-cu11 11.7.4.91\n",
      "    Uninstalling nvidia-cusparse-cu11-11.7.4.91:\n",
      "      Successfully uninstalled nvidia-cusparse-cu11-11.7.4.91\n",
      "  Attempting uninstall: nvidia-curand-cu11\n",
      "    Found existing installation: nvidia-curand-cu11 10.2.10.91\n",
      "    Uninstalling nvidia-curand-cu11-10.2.10.91:\n",
      "      Successfully uninstalled nvidia-curand-cu11-10.2.10.91\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu11\n",
      "    Found existing installation: nvidia-cuda-runtime-cu11 11.7.99\n",
      "    Uninstalling nvidia-cuda-runtime-cu11-11.7.99:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu11-11.7.99\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu11\n",
      "    Found existing installation: nvidia-cuda-cupti-cu11 11.7.101\n",
      "    Uninstalling nvidia-cuda-cupti-cu11-11.7.101:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu11-11.7.101\n",
      "  Attempting uninstall: nvidia-cublas-cu11\n",
      "    Found existing installation: nvidia-cublas-cu11 11.10.3.66\n",
      "    Uninstalling nvidia-cublas-cu11-11.10.3.66:\n",
      "      Successfully uninstalled nvidia-cublas-cu11-11.10.3.66\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.15\n",
      "    Uninstalling multiprocess-0.70.15:\n",
      "      Successfully uninstalled multiprocess-0.70.15\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.2\n",
      "    Uninstalling Jinja2-3.1.2:\n",
      "      Successfully uninstalled Jinja2-3.1.2\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.3.1\n",
      "    Uninstalling aiosignal-1.3.1:\n",
      "      Successfully uninstalled aiosignal-1.3.1\n",
      "  Attempting uninstall: responses\n",
      "    Found existing installation: responses 0.18.0\n",
      "    Uninstalling responses-0.18.0:\n",
      "      Successfully uninstalled responses-0.18.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.1\n",
      "    Uninstalling pandas-2.1.1:\n",
      "      Successfully uninstalled pandas-2.1.1\n",
      "  Attempting uninstall: nvidia-cusolver-cu11\n",
      "    Found existing installation: nvidia-cusolver-cu11 11.4.0.1\n",
      "    Uninstalling nvidia-cusolver-cu11-11.4.0.1:\n",
      "      Successfully uninstalled nvidia-cusolver-cu11-11.4.0.1\n",
      "  Attempting uninstall: nvidia-cudnn-cu11\n",
      "    Found existing installation: nvidia-cudnn-cu11 8.5.0.96\n",
      "    Uninstalling nvidia-cudnn-cu11-8.5.0.96:\n",
      "      Successfully uninstalled nvidia-cudnn-cu11-8.5.0.96\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.2\n",
      "    Uninstalling huggingface-hub-0.17.2:\n",
      "      Successfully uninstalled huggingface-hub-0.17.2\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.8.5\n",
      "    Uninstalling aiohttp-3.8.5:\n",
      "      Successfully uninstalled aiohttp-3.8.5\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.33.2\n",
      "    Uninstalling transformers-4.33.2:\n",
      "      Successfully uninstalled transformers-4.33.2\n",
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.5\n",
      "    Uninstalling datasets-2.14.5:\n",
      "      Successfully uninstalled datasets-2.14.5\n",
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: evaluate\n",
      "    Found existing installation: evaluate 0.4.0\n",
      "    Uninstalling evaluate-0.4.0:\n",
      "      Successfully uninstalled evaluate-0.4.0\n",
      "\u001b[33m  WARNING: The script evaluate-cli is installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.0.0\n",
      "    Uninstalling triton-2.0.0:\n",
      "      Successfully uninstalled triton-2.0.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.23.0\n",
      "    Uninstalling accelerate-0.23.0:\n",
      "      Successfully uninstalled accelerate-0.23.0\n",
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory and accelerate-launch are installed in '/home/anjinsung/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.1.3 accelerate-0.23.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 certifi-2023.7.22 charset-normalizer-3.2.0 cmake-3.27.5 colorama-0.4.6 datasets-2.14.5 dill-0.3.7 evaluate-0.4.0 filelock-3.12.4 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.17.2 idna-3.4 jinja2-3.1.2 lit-16.0.6 lxml-4.9.3 mpmath-1.3.0 multidict-6.0.4 multiprocess-0.70.15 networkx-3.1 numpy-1.26.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 packaging-23.1 pandas-2.1.1 portalocker-2.8.2 psutil-5.9.5 pyarrow-13.0.0 python-dateutil-2.8.2 pytz-2023.3.post1 pyyaml-6.0.1 regex-2023.8.8 requests-2.31.0 responses-0.18.0 sacrebleu-2.3.1 safetensors-0.3.3 sentencepiece-0.1.99 setuptools-68.2.2 six-1.16.0 sympy-1.12 tabulate-0.9.0 tokenizers-0.13.3 torch-2.0.1 tqdm-4.66.1 transformers-4.33.2 triton-2.0.0 typing-extensions-4.8.0 tzdata-2023.3 urllib3-2.0.5 wheel-0.41.2 xxhash-3.3.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall transformers transformers[torch] datasets sentencepiece evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0ff301-e376-49e8-a466-401c145170e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad18305-7d5b-4e3b-bff8-049525847ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595e68a5-727c-4331-bc44-f0b3606ca5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4548b5ed-e899-4072-83db-034b96b0dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1113_tech_train_set_1195228.json') as f:\n",
    "  js1 = json.loads(f.read())\n",
    "\n",
    "df1 = pd.DataFrame(js1['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "703b4973-911e-464e-a155-13a91d1e1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('기술과학분야_한영_train_set.json') as f:\n",
    "  js2 = json.loads(f.read())\n",
    "\n",
    "df2 = pd.DataFrame(js2['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f8d9e5-519e-42cd-ad4f-e06207239bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('일상생활및구어체_한영_train_set.json') as f:\n",
    "  js3 = json.loads(f.read())\n",
    "\n",
    "df3 = pd.DataFrame(js3['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913bacf4-52bf-49cb-9f3f-97e056ae1ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "koen1 = df1[['ko','en']]\n",
    "koen2 = df2[['ko','en']]\n",
    "koen3 = df3[['ko','en']]\n",
    "\n",
    "ko_en_df = pd.concat([koen1,koen2,koen3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bff8b31-fb00-4f8f-b8d6-c143d38d3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_en_df['ko'] = ko_en_df['ko'].str.replace(pat=r'[^\\w]', repl=r' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1986f1d6-2394-49ed-9587-c7c76d676b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_en_df['en'] = ko_en_df['en'].str.replace(pat=r'[^\\w]', repl=r' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7221c859-5e3f-4ce0-b030-589f41fc559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "415a7587-a099-48be-9dc8-f254b7605818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ko</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>오븐 세척제에 포함된 피부 부식성 자극성 물질은 모두 심한 눈손상성 눈 자극성에도 ...</td>\n",
       "      <td>All skin corrosion irritation substances conta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>국내에서는 2030년 BAU 대비 37 의 온실가스 감축을 의무화함으로써 이에 따른...</td>\n",
       "      <td>In Korea  it really is urgent to produce an an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>수술 전 정맥혈을 채취하고 월경주기의 난포기 동안 8시간 이상 금식한 후 정맥혈액을...</td>\n",
       "      <td>Before the operation  blood samples were obtai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3등급 이하 인정자는 8 2 만이 재가급여에서 시설급여로 변경하였다</td>\n",
       "      <td>Only 8 2  of third graders or lower changed fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>센싱에 의해 공기 중 오일입자가 기준치이하인지 여부를 판단할 수 있다</td>\n",
       "      <td>It is possible to determine whether or not the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595367</th>\n",
       "      <td>그럼 미디엄은 2리터 필요하신 거죠</td>\n",
       "      <td>So then you need 2 liters of the medium right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595368</th>\n",
       "      <td>환불 요청은 BBB 고객 지원으로 문의하십시오</td>\n",
       "      <td>Please contact BBB customer support for refund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595369</th>\n",
       "      <td>미니 사이즈로 언제어디서나 즐길 수 있는 휴대성을 자랑합니다</td>\n",
       "      <td>It is a mini size and boasts portability that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595370</th>\n",
       "      <td>본사에서 충분히 검토하고 선정했을 것이기에 서점에서 특별한 조치는 취하지 않을 것입니다</td>\n",
       "      <td>The bookstore will not take any special action...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595371</th>\n",
       "      <td>야이  씨</td>\n",
       "      <td>See</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3595372 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        ko  \\\n",
       "0        오븐 세척제에 포함된 피부 부식성 자극성 물질은 모두 심한 눈손상성 눈 자극성에도 ...   \n",
       "1        국내에서는 2030년 BAU 대비 37 의 온실가스 감축을 의무화함으로써 이에 따른...   \n",
       "2        수술 전 정맥혈을 채취하고 월경주기의 난포기 동안 8시간 이상 금식한 후 정맥혈액을...   \n",
       "3                   3등급 이하 인정자는 8 2 만이 재가급여에서 시설급여로 변경하였다    \n",
       "4                  센싱에 의해 공기 중 오일입자가 기준치이하인지 여부를 판단할 수 있다    \n",
       "...                                                    ...   \n",
       "3595367                               그럼 미디엄은 2리터 필요하신 거죠    \n",
       "3595368                         환불 요청은 BBB 고객 지원으로 문의하십시오    \n",
       "3595369                 미니 사이즈로 언제어디서나 즐길 수 있는 휴대성을 자랑합니다    \n",
       "3595370  본사에서 충분히 검토하고 선정했을 것이기에 서점에서 특별한 조치는 취하지 않을 것입니다    \n",
       "3595371                                             야이  씨    \n",
       "\n",
       "                                                        en  \n",
       "0        All skin corrosion irritation substances conta...  \n",
       "1        In Korea  it really is urgent to produce an an...  \n",
       "2        Before the operation  blood samples were obtai...  \n",
       "3        Only 8 2  of third graders or lower changed fr...  \n",
       "4        It is possible to determine whether or not the...  \n",
       "...                                                    ...  \n",
       "3595367     So then you need 2 liters of the medium right   \n",
       "3595368  Please contact BBB customer support for refund...  \n",
       "3595369  It is a mini size and boasts portability that ...  \n",
       "3595370  The bookstore will not take any special action...  \n",
       "3595371                                               See   \n",
       "\n",
       "[3595372 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_en_df = ko_en_df.reset_index(drop=True)\n",
    "\n",
    "ko_en_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b871c1-64e2-442f-b8d0-8b48f1be558e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ko', 'en'],\n",
       "    num_rows: 3595372\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(ko_en_df)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9447ed4f-b425-4612-89e5-83b94088ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is already `map`'d and already has `set_format`\n",
    "# 90% train, 10% test + validation\n",
    "train_testvalid = dataset.train_test_split(test_size=0.1)\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_testvalid[\"train\"],\n",
    "    \"test\": test_valid[\"test\"],\n",
    "    \"valid\": test_valid[\"train\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5609190c-5801-4efc-92b0-f56c9e53f39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ko', 'en'],\n",
       "        num_rows: 3235834\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ko', 'en'],\n",
       "        num_rows: 179769\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['ko', 'en'],\n",
       "        num_rows: 179769\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb4a6781-3e27-4ec7-85e5-1c5bc41f6ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855848ffa4394da38da8c2ec5c3e85c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/3235834 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235f475e5b194589bcda222270be87c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/179769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c44ed36a3c4baaa5c8188da27d9ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/179769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets.save_to_disk('koentalktech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528e16a2-16ae-4f43-a7db-329a977c6cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78e773c8-5bf1-410b-a6c7-54b86e471ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67440b00-38a6-41fa-a174-12781b6fdfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86d72e35-f56d-4933-b064-174106c409bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "714a979f-8267-4218-ba16-279fa26d0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "278e5a57-8214-4667-8d01-fa649b5935ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da26a2c8-37f4-4d3a-8169-c5b00fc7fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"KETI-AIR/ke-t5-base\"\n",
    "max_token_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72d22225-ac90-445b-b8a8-6147fd772df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6906b746-dc5f-45cf-ae86-4fe4623abfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples['ko']\n",
    "    targets = examples['en']\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_token_length, truncation=True\n",
    "        )\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b78c8900-9ed2-4800-bed5-98661a4d3f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CPU = multiprocessing.cpu_count()\n",
    "NUM_CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325e67c-362f-4412-8ab2-9a20722c43f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(preprocess_function,\n",
    "                                 batched=True,\n",
    "                                 remove_columns=datasets[\"train\"].column_names,\n",
    "                                 num_proc=NUM_CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8122e2-04d1-48f7-87f1-906988a15c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.save_to_disk('koentalktech_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c06abc7-ae69-4ec7-91b3-53e33faac4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_datasets = load_from_disk('koentalktech_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09244a85-98c9-4e99-bad2-7aa4bfb6a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b798c8a-41a3-4733-8d51-d9c62645329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7cffb6e-76b5-4984-9946-85e9589d1fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3235834\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 179769\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 179769\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56cca881-2c88-4ec8-917d-c428d5ecd902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36c79d2f-51bc-4541-b14c-a8f95d794459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73351b99-b55f-461e-9f87-6c8a4092c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"chkpt\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    gradient_accumulation_steps=2,\n",
    "    report_to=\"none\" # Wandb 로그 끄기\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc6ce04f-ae6b-4c03-8cc0-7f54c4d91b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff28b934-fca5-4f22-be07-ab63287661cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='37920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    7/37920 00:09 < 19:00:55, 0.55 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 23.69 GiB total capacity; 21.85 GiB already allocated; 2.06 MiB free; 22.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1835\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1835\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1838\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1840\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1841\u001b[0m ):\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2679\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2679\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2682\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2704\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2703\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2704\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2705\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2706\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:172\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:184\u001b[0m, in \u001b[0;36mDataParallel.gather\u001b[0;34m(self, outputs, output_device)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgather\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs, output_device):\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:86\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(outputs, target_device, dim)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Recursive function calls like this create reference cycles.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Setting the function to None clears the refcycle.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mgather_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     gather_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:77\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m outputs):\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll dicts must have the same number of keys\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgather_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)\u001b[38;5;241m.\u001b[39m_make(\u001b[38;5;28mmap\u001b[39m(gather_map, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39moutputs)))\n",
      "File \u001b[0;32m<string>:12\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, loss, logits, past_key_values, decoder_hidden_states, decoder_attentions, cross_attentions, encoder_last_hidden_state, encoder_hidden_states, encoder_attentions)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py:343\u001b[0m, in \u001b[0;36mModelOutput.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# if we provided an iterator as first field and the iterator is a (key, value) iterator\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# set the associated fields\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_field_iterator:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(iterator):\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    345\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(element, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[1;32m    346\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(element) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    347\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(element[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    348\u001b[0m         ):\n\u001b[1;32m    349\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    350\u001b[0m                 \u001b[38;5;66;03m# If we do not have an iterator of key/values, set it as attribute\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:77\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m outputs):\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll dicts must have the same number of keys\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)((k, \u001b[43mgather_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     78\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m out)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)\u001b[38;5;241m.\u001b[39m_make(\u001b[38;5;28mmap\u001b[39m(gather_map, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39moutputs)))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:81\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)\u001b[38;5;241m.\u001b[39m_make(\u001b[38;5;28mmap\u001b[39m(gather_map, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39moutputs)))\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgather_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:81\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)\u001b[38;5;241m.\u001b[39m_make(\u001b[38;5;28mmap\u001b[39m(gather_map, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39moutputs)))\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgather_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:71\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     69\u001b[0m out \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGather\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:75\u001b[0m, in \u001b[0;36mGather.forward\u001b[0;34m(ctx, target_device, dim, *inputs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     ctx\u001b[38;5;241m.\u001b[39munsqueezed_scalar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     74\u001b[0m ctx\u001b[38;5;241m.\u001b[39minput_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(i\u001b[38;5;241m.\u001b[39msize(ctx\u001b[38;5;241m.\u001b[39mdim) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/comm.py:235\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(tensors, dim, destination, out)\u001b[0m\n\u001b[1;32m    231\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    232\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing -1 to represent CPU tensor is deprecated. Please use a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice object or string instead, e.g., \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    234\u001b[0m     destination \u001b[38;5;241m=\u001b[39m _get_device_index(destination, allow_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m destination \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 23.69 GiB total capacity; 21.85 GiB already allocated; 2.06 MiB free; 22.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6562b18c-5a09-4be5-9dde-d7e74a5aa0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "396bd194-a45c-4377-8a9a-bf13cead1a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./chkpt/checkpoint-37920\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "\n",
    "model.cpu();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18555eb3-f691-4bb2-800a-12a838b8536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [\n",
    "    \"이번시간에는 선형회귀 알고리즘을 수학적으로 이해해보도록 하겠습니다.\",\n",
    "    \"앞서, 살펴본 바와 같이, 선형회귀 모델은 선입니다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9563559-726f-4a36-bba3-1aca92783507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjinsung/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(input_text, return_tensors=\"pt\", \n",
    "                   padding=True, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e5badc-9d84-46ee-a06d-f73ee0ad223e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  160,   669,   202,   805,   338,   290,  2919, 30044,    11,  7020,\n",
       "           551,  2292,    80,   518,  1396, 12584,     3,     1],\n",
       "        [  445,     4, 36513, 28902,   875,     4,   805,   338,   290,  2919,\n",
       "         22276,   805,   335,     3,     1,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b20c36-4715-4095-bdc5-2d76e472f76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng = model.generate(\n",
    "    **inputs,\n",
    "    max_length=64,\n",
    "    num_beams=5,\n",
    ")\n",
    "\n",
    "eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150d59bc-287f-4473-9d0f-e6db89a2a4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> In this time we will mathematically understand the linear regression algorithm </s>',\n",
       " '<pad> As seen above the linear regression model is a line </s><pad><pad>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ \n",
    "    tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(e)) for e in eng\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c49a80ce-7ef7-4e93-bead-2c883fec9b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_datasets = load_from_disk('koentalktech_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ce2139b-bdf3-45d7-8512-f899501f27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], batch_size=32, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "582b2213-8d7d-4bfb-a1e9-ab4d459378d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader_iter = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b8fa1a0-c6f5-44bb-a8e3-ee1007684124",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(test_dataloader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8361bdb1-98e9-4665-a1e6-fd46844887c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17fd7736-3a61-4767-82f0-e824aa4c7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = { key: test_batch[key] for key in ('input_ids', 'attention_mask') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edadda00-17c3-4501-bd77-1beac2ef2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "englishs = model.generate(\n",
    "    **test_input,\n",
    "    max_length=64,\n",
    "    num_beams=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "784639fa-e276-49b3-9884-edccad73890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =  np.where(test_batch.labels != -100, test_batch.labels, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "537ad5d8-847e-4db3-882d-d964f1472743",
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_sents = tokenizer.batch_decode(test_batch.input_ids, skip_special_tokens=True)[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae5a5491-b7ef-4eb4-b2d6-3ad4285c6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = tokenizer.batch_decode(labels, skip_special_tokens=True)[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52a08ce1-9b9b-4212-b2ef-8b635449fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tokenizer.batch_decode(englishs, skip_special_tokens=True )[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2696179c-11fa-4ec4-bc8b-ac3d33fd2254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean   : 이미 발굴 유물로 인정된 이 동전 무더기의 가치는 공식적으로 산정되지는 않았지만 100만 파운드 이상 나갈 수 있다고 이 매체는 전문가들을 인용해 전했다 \n",
      "Reference : The value of this sultry coin already recognized as an excavation artifact has not been officially calculated but it could go out for more than 1 million the media reported citing experts \n",
      "Translated: The value of the plurality of coins that have already been recognized as a excavation artifact is not officially calculated but it can take more than 100 million dollars according to experts \n",
      "\n",
      "\n",
      "Korean   : 그 이유는 서로 다른n p형 반도체를 합성하게 되면 각 필름들의 다수 캐리어 농도의 영향이 생긴 것으로 판단된다 \n",
      "Reference : The reason is that when different n and p type semiconductors are synthesized it is judged that the influence of the concentration of multiple carriers in each film occurs \n",
      "Translated: The reason is that when the differentn p type semiconductor is synthesized it is judged that the effect of a large number of carrier concentrations of each film has occurred \n",
      "\n",
      "\n",
      "Korean   : 저도 오늘 6시에 기상했습니다 \n",
      "Reference : I also woke up at 6 o clock today \n",
      "Translated: I ve arrived at 6 p m today \n",
      "\n",
      "\n",
      "Korean   : EU 대외관계청 EEAS 은 중아공에 곧 EU 병력을 파견할 것이라고 29일 밝혔다 \n",
      "Reference : The EUForeign Relations Agency EEAS announced on the 29th that it would soon send EUtroops to Central Africa \n",
      "Translated: The EU Foreign Affairs Agency EEAS announced on the 29th that it would send EU troops to China soon \n",
      "\n",
      "\n",
      "Korean   : 의료 어플리케이션은 서로 다른 의료 데이터를 제공할 수 있다 \n",
      "Reference : Medical applications may provide different pieces of medical data \n",
      "Translated: The medical application may provide different medical data \n",
      "\n",
      "\n",
      "Korean   : 이러한 세 가지 주된 논거를 아래에서 좀 더 구체적으로 검토하겠다 \n",
      "Reference : These three main arguments will be considered in more detail below \n",
      "Translated: The three main arguments will be reviewed more detail below \n",
      "\n",
      "\n",
      "Korean   : 중국 당국은 오염 유발 공장 2천 곳의 가동을 전면 중단시켰다 \n",
      "Reference : 2 000 polluting factories in China have been shut down by the Chinese government \n",
      "Translated: China s authorities completely halted the operation of 2 000 contaminated factories \n",
      "\n",
      "\n",
      "Korean   : 이는 가톨릭 교회가 지난 1960년대 당시 죽음을 심장의 박동이 멈추는 것이 아닌 뇌의 활동 정지라고 정의한 BBB의 발표를 받아들여 지금까지 뇌사자의 장기이식을 인정해온 것과 정면 배치되는 것이어서 주목된다 \n",
      "Reference : This is noteworthy because the Catholic Church in the 1960s accepted the announcement of the BBB which defined death as the cessation of brain activity rather than the cessation of the heartbeat and it is in direct opposition to the recognition of organ transplantation from the brain dead \n",
      "Translated: This is significant because the Catholic Church has passed out the BBB s statement that death is a stop of the heart in the 1960s and has recognized the transplantation of the brain dead so far due to the acceptance of the transplantation of the brain \n",
      "\n",
      "\n",
      "Korean   : 이중 흰점박이 꽃무지는 최적화 조건에서 발효되어진 톱밥이 사육에 대량사용 되어지는 어려움이 있어 아직 생산 단가를 맞추기가 쉽지 않은 실정이다 \n",
      "Reference : Among them it is difficult to meet the production cost of white spotted flower radish because sawdust fermented under optimized conditions is used in large quantities for breeding \n",
      "Translated: Among them the white phenomena flower flower is not yet easy to meet the production price due to the difficulties of mass use of hot rice induced in optimized conditions \n",
      "\n",
      "\n",
      "Korean   : 낮은 비 가설 관류 선량은 급성 질환 환자의 코티솔 수준을 정상화할 수 있으며 제어되지 않는 초코르티솔리즘이 있는 CD 환자들의 경우 수술 전 코티솔 수준을 정규화할 수 있다 \n",
      "Reference : A low non hypnotic perfusion dose can normalize cortisol levels in patients acutely ill or preoperatively in CD patients with uncontrolled hypercortisolism \n",
      "Translated: The low non hypothesis residual amount may normalize the coronasol level in acute disease patients and in CD patients with uncontrolled hypercococococococococococococococococococococococococococococococo\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in zip(kor_sents, references, preds):\n",
    "    print('Korean   :', s[0])\n",
    "    print('Reference :', s[1])\n",
    "    print('Translated:', s[2])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f40c6f7-d021-4ed2-972c-d2a7a30c692f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
